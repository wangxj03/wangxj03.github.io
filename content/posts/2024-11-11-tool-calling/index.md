---
title: "Tips to Improve LLM Tool Calling"
date: 2024-11-10T20:37:57-08:00
tags: ["AI"]
author: "Xiaojing Wang"
showToc: true
TocOpen: false
draft: true
hidemeta: false
comments: false
disableHLJS: false
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: false
UseHugoToc: true
---

LLM tool calling is a vital step in building agentic AI workflows. In this post, we demonstrate tips to improve LLM tool calling by building a practical text-to-SQL application using [DuckDB](https://github.com/duckdb/duckdb).

## Building a Text-to-SQL Application

We will create a simple text-to-SQL application using the [Hacker news](https://motherduck.com/docs/getting-started/sample-data-queries/hacker-news/) dataset compiled by [Mother Duck](https://motherduck.com/). The dataset contains user posts, comments, and votes from most of 2022. For simplicity, we only use story posts with non-null text.

The application provides a natural language interface to the dataset, translating user input into SQL queries, executing them against a DuckDB database, and returning the JSON results in natural language.

The core function, `run_sql`, is responsible for executing SQL queries:

```python
import duckdb
from duckdb import DuckDBPyConnection

def get_connection() -> DuckDBPyConnection:
    global con
    if con is None:
        con = duckdb.connect(":memory:")
        con.execute("""
CREATE TABLE IF NOT EXISTS posts AS
SELECT * FROM read_parquet('data/hacker_news.parquet');
""")
    return con


def run_sql(query: str) -> str:
    """Run DuckDB SQL query against Hacker news table `posts` and return the result as a JSON string."""

    con = get_connection()
    df = con.sql(query).fetchdf()

    # Truncate the result if it's too long
    if len(df) > 100:
        logging.warning(
            f"The result contains {len(df)} rows. Only returning the first 100."
        )
        df = df.head(100)

    return df.to_json(orient="records")
```

### Workflow Overview

The application follows this lifecycle:

![alt text](function_call.png)

### Success Example

For a query like "What are the most commented posts?", the LLM translates it into SQL:

```sql
SELECT title, comments FROM posts ORDER BY comments DESC LIMIT 10;
```

It then returns the result in a formatted markdown list:

![alt text](most_commented.png)

### Failure Example

While the application handles simple queries well, it struggles with more complex ones like "What are the most commented posts each month?". This requires advanced SQL features like subqueries. Below are examples of incorrect SQL generated by the LLM:

- Incorrect `HAVING` clause:

  ```sql
  SELECT strftime('%Y-%m', timestamp) as month, MAX(comments) as max_comments, title
  FROM posts
  GROUP BY month, title
  HAVING comments = (SELECT MAX(comments) FROM posts WHERE strftime('%Y-%m', timestamp) = month)
  ORDER BY month;
  ```

- Unnecessary outer query

  ```sql
  SELECT strftime(timestamp, '%Y-%m') as month, title, comments
  FROM (
  SELECT month, title, comments,
  ROW_NUMBER() OVER (PARTITION BY month ORDER BY comments DESC) as rn
  FROM (
      SELECT strftime(timestamp, '%Y-%m') as month, title, comments
      FROM posts
  )
  )
  WHERE rn = 1;
  ```

## Tips to Improve LLM Tool Calling

### 1. Retry on Error

LLMs often fail on the first attempt with complex tasks but perform better when given feedback. Retrying with error messages and additional context allows the LLM to refine its response.

We witnessed this in the recent [computer use](https://www.anthropic.com/news/3-5-models-and-computer-use) for coding demo by Anthropic. As part of the coding task to build a new personal website, Claude first attempted to start a Python server with an error, but then retried with a new version and succeeded.

![alt text](claude_computer_use_for_coding.png)

For example, given the query "What are the most commented posts each month?", the initial SQL is invalid:

```sql
SELECT STRFTIME(timestamp, '%Y-%m') AS month, title, MAX(comments) as max_comments
FROM posts
GROUP BY month
ORDER BY month;
```

The `run_sql` function throws an error:

```sh
Binder Error: column "title" must appear in the GROUP BY clause or must be part of an aggregate function. Either add it to the GROUP BY list, or use "ANY_VALUE(title)" if the exact value of "title" is not important.
```

By sending the error message back to the LLM along with the original query, the model has the opportunity to refine its response. In this case, the LLM generates a corrected query:

```sql
SELECT month, title, comments FROM (
    SELECT STRFTIME(timestamp, '%Y-%m') AS month, title, comments,
           ROW_NUMBER() OVER (PARTITION BY STRFTIME(timestamp, '%Y-%m') ORDER BY comments DESC) as rn
    FROM posts
) WHERE rn = 1;
```

### 2. Allow More Thinking Time

There has been a lot of recent talks about the inference scaling law which suggests the performance of a model improves with more computing time spent on inference. Jim Fan [tweeted](https://x.com/DrJimFan/status/1834279865933332752) about it when the OpenAI o1 model was out. Sequoia published a blog post [Generative AIâ€™s Act o1](https://www.sequoiacap.com/article/generative-ais-act-o1/) to look into the future with its implications.

How does this apply to our text-to-SQL application? One simple approach to allow more thinking time is to a pre-tool-calling step. In this step, the model is instructed to first break down the task into logical steps and draft SQL for each step before proceeding to the tool calling and execution.

Consider the same input query "What are the most commented posts each month?". By adding a reasoning step, the model can sketch out a plan to improve the SQL generation. Here's an example captured in the Langfuse [trace](https://us.cloud.langfuse.com/project/cm27ro2si00cd8mi56o0af4bq/traces/69bd6cfc-c8b6-4960-a3ef-08d6f4b06a73), where the model explicitly reasons through the solution

For example, instead of directly generating SQL for the query "What are the most commented posts each month?", instruct the model to first break the task into logical steps.

> 1. **Extract the year and month** from the `timestamp` to group the data on a monthly basis.
> 2. **Rank the posts** within each month based on the number of comments to identify the most commented one.
> 3. **Filter** the top-ranked post for each month.

This reasoning enables the LLM to produce an accurate final SQL query, even it is slightly verbose:

```sql
WITH posts_with_month AS (
    SELECT
        title,
        comments,
        EXTRACT(YEAR FROM timestamp) AS year,
        EXTRACT(MONTH FROM timestamp) AS month
    FROM
        posts
),
ranked_posts AS (
    SELECT
        title,
        comments,
        year,
        month,
        ROW_NUMBER() OVER (PARTITION BY year, month ORDER BY comments DESC) AS rank
    FROM
        posts_with_month
)
SELECT
    title,
    comments,
    year,
    month
FROM
    ranked_posts
WHERE
    rank = 1
ORDER BY
    year, month;
```

### 3. Use Few-shot Learning

Providing relevant examples in the prompt can dramatically improve tool calling. DuckDB's [documentation](https://duckdb.org/docs/sql/introduction) contains many SQL snippets that can be leveraged as examples. We leverage the [instructor](https://github.com/instructor-ai/instructor) package to parse these examples into structured output. The Pydantic field descriptions serve as prompts for the LLM. We then index the example embeddings into a vector database and retrieve the most similar examples at inference time.

```python
class Example(BaseModel):
    task: str = Field(
        ..., description="Description of the task that is to be solved by the SQL query"
    )
    sql: str = Field(..., description="DuckDB SQL query to solve the task")
    explanation: str = Field(
        ...,
        description="Generic explanation of the query syntax found in the surrounding markdown",
    )
```

For the same input query "What are the most commented posts each month?", this Langfuse [trace](https://us.cloud.langfuse.com/project/cm27ro2si00cd8mi56o0af4bq/traces/e8956c34-6569-4324-ad4e-3b0be153b9e2) shows that the model retrieved 10 most similar examples with [one](https://duckdb.org/docs/sql/query_syntax/qualify.html#examples) for the `QUALIFY` clause. The LLM then learned to generate a correct SQL query using the `QUALIFY` clause, which is the most succinct and efficient way to solve the problem.

```sql
SELECT DATE_TRUNC('month', timestamp) as month, title, comments
FROM posts
WHERE comments IS NOT NULL
QUALIFY ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', timestamp) ORDER BY comments DESC) = 1;
```

## Bonus: Observability with LLM Tracing

LLM observability tools like [Langfuse](https://langfuse.com/), [Langsmith](https://www.langchain.com/langsmith), and [braintrust](https://www.braintrust.dev/docs/guides/tracing) allow you to monitor LLM's reasoning process and interactions with tools. Traces capture the input and output of LLM API calls, vector database API calls, and local function calls. They help you diagnose and optimize performance. You can also measure the latency for each step as well as the end-to-end latency.

## Conclusion

This post showed a few tips to enhance LLM tool calling using a text-to-SQL application. By enabling retries, incorporating reasoning steps, and using few-shot learning, you can significantly improve LLM tool calling.
