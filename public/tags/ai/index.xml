<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Xiaojing&#39;s Blog</title>
    <link>https://example.org/tags/ai/</link>
    <description>Recent content in AI on Xiaojing&#39;s Blog</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Aug 2024 17:25:22 -0700</lastBuildDate>
    <atom:link href="https://example.org/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Design Pattern: Guardrails</title>
      <link>https://example.org/posts/2024-08-18-guardrails/</link>
      <pubDate>Sun, 18 Aug 2024 17:25:22 -0700</pubDate>
      <guid>https://example.org/posts/2024-08-18-guardrails/</guid>
      <description>Guardrails are controls designed to prevent inappropriate content from reaching or being generated by AI systems. There are two main types: input guardrails and output guardrails. Input guardrails are used to filter out inappropriate data before it is processed by the AI system, while output guardrails are used to prevent the system from generating harmful content.
We adapt an example from OpenAI cookbook how to use guardrails. We develop a GenAI chatbot with both input and output guardrails.</description>
    </item>
  </channel>
</rss>
