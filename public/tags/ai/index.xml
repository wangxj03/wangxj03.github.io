<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on Xiaojing&#39;s Blog</title>
    <link>https://example.org/tags/ai/</link>
    <description>Recent content in AI on Xiaojing&#39;s Blog</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Aug 2024 17:25:22 -0700</lastBuildDate>
    <atom:link href="https://example.org/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Design Pattern: Guardrails</title>
      <link>https://example.org/posts/2024-08-18-guardrails/</link>
      <pubDate>Sun, 18 Aug 2024 17:25:22 -0700</pubDate>
      <guid>https://example.org/posts/2024-08-18-guardrails/</guid>
      <description>This is bold text, and this is emphasized text.
Visit the Hugo website!
Guardrails Guardrails are a set of constraints that guide the behavior of AI systems. They are designed to prevent the system from making harmful decisions. Guardrails can be implemented at different levels of the AI system, including data, model, and decision-making stages. For example, data guardrails can be used to filter out biased data, model guardrails can be used to prevent overfitting, and decision-making guardrails can be used to ensure that the system follows ethical guidelines.</description>
    </item>
  </channel>
</rss>
