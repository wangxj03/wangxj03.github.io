---
title: "Tips to Improve LLM Tool Calling"
date: 2024-11-10T20:37:57-08:00
tags: ["AI"]
author: "Xiaojing Wang"
showToc: true
TocOpen: false
draft: true
hidemeta: false
comments: false
disableHLJS: false
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: false
UseHugoToc: true
---

LLM tool calling is a vital step in building agentic AI workflows. In this post, we demonstrate tips to improve LLM tool calling by building a practical text-to-SQL application using [DuckDB](https://github.com/duckdb/duckdb).

## Building a Text-to-SQL Application

We will create a simple text-to-SQL application using the [Hacker news](https://motherduck.com/docs/getting-started/sample-data-queries/hacker-news/) dataset compiled by [Mother Duck](https://motherduck.com/). The dataset contains user posts, comments, and votes from most of 2022. For simplicity, we only use story posts with non-null text.

The application provides a natural language interface to the dataset, translating user input into SQL queries, executing them against a DuckDB database, and returning the JSON results in natural language.

The core function, `run_sql`, is responsible for executing SQL queries:

```python
import duckdb
from duckdb import DuckDBPyConnection

def get_connection() -> DuckDBPyConnection:
    global con
    if con is None:
        con = duckdb.connect(":memory:")
        con.execute("""
CREATE TABLE IF NOT EXISTS posts AS
SELECT * FROM read_parquet('data/hacker_news.parquet');
""")
    return con


def run_sql(query: str) -> str:
    """Run DuckDB SQL query against Hacker news table `posts` and return the result as a JSON string."""

    con = get_connection()
    df = con.sql(query).fetchdf()

    # Truncate the result if it's too long
    if len(df) > 100:
        logging.warning(
            f"The result contains {len(df)} rows. Only returning the first 100."
        )
        df = df.head(100)

    return df.to_json(orient="records")
```

### Workflow Overview

The application follows this lifecycle:

![alt text](function_call.png)

### Success Example

For a query like "What are the most commented posts?", the LLM translates it into SQL:

```sql
SELECT title, comments FROM posts ORDER BY comments DESC LIMIT 10;
```

It then returns the result in a formatted markdown list:

![alt text](most_commented.png)

### Failure Example

While the application handles simple queries well, it struggles with more complex ones like "What are the most commented posts each month?". This requires advanced SQL features like subqueries. Below are examples of incorrect SQL generated by the LLM:

- Incorrect `HAVING` clause:

  ```sql
  SELECT strftime('%Y-%m', timestamp) as month, MAX(comments) as max_comments, title
  FROM posts
  GROUP BY month, title
  HAVING comments = (SELECT MAX(comments) FROM posts WHERE strftime('%Y-%m', timestamp) = month)
  ORDER BY month;
  ```

- Unnecessary outer query

  ```sql
  SELECT strftime(timestamp, '%Y-%m') as month, title, comments
  FROM (
  SELECT month, title, comments,
  ROW_NUMBER() OVER (PARTITION BY month ORDER BY comments DESC) as rn
  FROM (
      SELECT strftime(timestamp, '%Y-%m') as month, title, comments
      FROM posts
  )
  )
  WHERE rn = 1;
  ```

## Tips to Improve LLM Tool Calling

### 1. Retry on Error

LLM tool calling often fails on the first attempt, especially with complex tasks. However, by providing error feedback and retrying, LLMs can refine their output. A recent example of this iterative approach comes from Anthropic's [computer use](https://www.anthropic.com/news/3-5-models-and-computer-use) coding demo. For the coding task of building a new website, Claude initially failed to start a Python server, it retried and successfully completing the task

![alt text](claude_computer_use_for_coding.png)

Consider the same query "What are the most commented posts each month?". Initially, the LLM generates this incorrect SQL:

```sql
SELECT STRFTIME(timestamp, '%Y-%m') AS month, title, MAX(comments) as max_comments
FROM posts
GROUP BY month
ORDER BY month;
```

Executing the query against the DuckDB database returns an error:

```sh
Binder Error: column "title" must appear in the GROUP BY clause or must be part of an aggregate function. Either add it to the GROUP BY list, or use "ANY_VALUE(title)" if the exact value of "title" is not important.
```

By providing the error message and the original query as feedback, the LLM corrects its output:

```sql
SELECT month, title, comments FROM (
    SELECT STRFTIME(timestamp, '%Y-%m') AS month, title, comments,
           ROW_NUMBER() OVER (PARTITION BY STRFTIME(timestamp, '%Y-%m') ORDER BY comments DESC) as rn
    FROM posts
) WHERE rn = 1;
```

### 2. Allow More Thinking Time

The concept of the **inference scaling law** has been gaining traction recently, popularized by AI influencer Jim Fan's [tweet](https://x.com/DrJimFan/status/1834279865933332752) and Sequoia capital's blog post [Generative AIâ€™s Act o1](https://www.sequoiacap.com/article/generative-ais-act-o1/). The law suggests that the performance of a model improves with more computing time spent on inference.

How does this apply to our text-to-SQL application? We can simply introduce a pre-tool-calling reasoning step by instructing the model to explicity break down the task to logical steps, and then invoke the tool for execution.

We again concider the same query "What are the most commented posts each month?". This Langfuse [trace](https://us.cloud.langfuse.com/project/cm27ro2si00cd8mi56o0af4bq/traces/69bd6cfc-c8b6-4960-a3ef-08d6f4b06a73) reveals that the task was broken into 3 logical steps during reasoning:

1. **Extract the year and month** from the `timestamp` to group the data on a monthly basis.
2. **Rank the posts** within each month based on the number of comments to identify the most commented one.
3. **Filter** the top-ranked post for each month.

This structured approach leads to a correct final SQL query, even being slightly verbose:

```sql
WITH posts_with_month AS (
    SELECT
        title,
        comments,
        EXTRACT(YEAR FROM timestamp) AS year,
        EXTRACT(MONTH FROM timestamp) AS month
    FROM
        posts
),
ranked_posts AS (
    SELECT
        title,
        comments,
        year,
        month,
        ROW_NUMBER() OVER (PARTITION BY year, month ORDER BY comments DESC) AS rank
    FROM
        posts_with_month
)
SELECT
    title,
    comments,
    year,
    month
FROM
    ranked_posts
WHERE
    rank = 1
ORDER BY
    year, month;
```

### 3. Use Few-shot Learning

Providing relevant examples in the prompt can dramatically improve tool calling. DuckDB's [documentation](https://duckdb.org/docs/sql/introduction) contains many SQL snippets that can be leveraged as examples. We leverage the [instructor](https://github.com/instructor-ai/instructor) package to parse these examples into structured output. The Pydantic field descriptions serve as prompts for the LLM. We then index the example embeddings into a vector database and retrieve the most similar examples at inference time.

```python
class Example(BaseModel):
    task: str = Field(
        ..., description="Description of the task that is to be solved by the SQL query"
    )
    sql: str = Field(..., description="DuckDB SQL query to solve the task")
    explanation: str = Field(
        ...,
        description="Generic explanation of the query syntax found in the surrounding markdown",
    )
```

For the same input query "What are the most commented posts each month?", this Langfuse [trace](https://us.cloud.langfuse.com/project/cm27ro2si00cd8mi56o0af4bq/traces/e8956c34-6569-4324-ad4e-3b0be153b9e2) shows that the model retrieved 10 most similar examples with [one](https://duckdb.org/docs/sql/query_syntax/qualify.html#examples) for the `QUALIFY` clause. The LLM then learned to generate a correct SQL query using the `QUALIFY` clause, which is the most succinct and efficient way to solve the problem.

```sql
SELECT DATE_TRUNC('month', timestamp) as month, title, comments
FROM posts
WHERE comments IS NOT NULL
QUALIFY ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', timestamp) ORDER BY comments DESC) = 1;
```

## Bonus: Observability with LLM Tracing

LLM observability tools like [Langfuse](https://langfuse.com/), [Langsmith](https://www.langchain.com/langsmith), and [braintrust](https://www.braintrust.dev/docs/guides/tracing) allow you to monitor LLM's reasoning process and interactions with tools. Traces capture the input and output of LLM API calls, vector database API calls, and local function calls. They help you diagnose and optimize performance. You can also measure the latency for each step as well as the end-to-end latency.

## Conclusion

This post showed a few tips to enhance LLM tool calling using a text-to-SQL application. By enabling retries, incorporating reasoning steps, and using few-shot learning, you can significantly improve LLM tool calling.
