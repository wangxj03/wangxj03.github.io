<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Xiaojing&#39;s Blog</title>
    <link>https://example.org/posts/</link>
    <description>Recent content in Posts on Xiaojing&#39;s Blog</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Aug 2024 17:25:22 -0700</lastBuildDate>
    <atom:link href="https://example.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Design Pattern: Guardrails</title>
      <link>https://example.org/posts/2024-08-18-guardrails/</link>
      <pubDate>Sun, 18 Aug 2024 17:25:22 -0700</pubDate>
      <guid>https://example.org/posts/2024-08-18-guardrails/</guid>
      <description>Guardrails in AI are essential controls that ensure AI systems do not generate or process inappropriate content. These controls fall into two categories: input guardrails and output guardrails. Input guardrails filter out unsuitable data before it reaches the AI system, while output guardrails prevent the AI from producing harmful content.
In this post, weâ€™ll build on an example from the OpenAI cookbook to create a GenAI chatbot equipped with both input and output guardrails.</description>
    </item>
  </channel>
</rss>
