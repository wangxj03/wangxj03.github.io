<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Semantic Code Search | Xiaojing's Blog</title>
<meta name=keywords content="AI"><meta name=description content="There’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let&rsquo;s dive in.
Understanding Cursor&rsquo;s Magic Cursor&rsquo;s codebase indexing, as explained in this forum post, works as follows:
It chunks your codebase files locally.
These chunks are then sent to Cursor&rsquo;s server, where embeddings are created using either OpenAI&rsquo;s embedding API or a custom embedding model."><meta name=author content="Xiaojing Wang"><link rel=canonical href=http://localhost:1313/posts/2024-09-24-code-search/><meta name=google-site-verification content="G-NX22EB1PRP"><link crossorigin=anonymous href=/assets/css/stylesheet.e7c811b1152f0ea0017b0724f2c040700cf8bf84343fd4fd5eca45af82337db9.css integrity="sha256-58gRsRUvDqABewck8sBAcAz4v4Q0P9T9XspFr4Izfbk=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2024-09-24-code-search/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Semantic Code Search"><meta property="og:description" content="There’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let&rsquo;s dive in.
Understanding Cursor&rsquo;s Magic Cursor&rsquo;s codebase indexing, as explained in this forum post, works as follows:
It chunks your codebase files locally.
These chunks are then sent to Cursor&rsquo;s server, where embeddings are created using either OpenAI&rsquo;s embedding API or a custom embedding model."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2024-09-24-code-search/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-24T23:01:05-07:00"><meta property="article:modified_time" content="2024-09-24T23:01:05-07:00"><meta property="og:site_name" content="Xiaojing's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Semantic Code Search"><meta name=twitter:description content="There’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let&rsquo;s dive in.
Understanding Cursor&rsquo;s Magic Cursor&rsquo;s codebase indexing, as explained in this forum post, works as follows:
It chunks your codebase files locally.
These chunks are then sent to Cursor&rsquo;s server, where embeddings are created using either OpenAI&rsquo;s embedding API or a custom embedding model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Semantic Code Search","item":"http://localhost:1313/posts/2024-09-24-code-search/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Semantic Code Search","name":"Semantic Code Search","description":"There’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let\u0026rsquo;s dive in.\nUnderstanding Cursor\u0026rsquo;s Magic Cursor\u0026rsquo;s codebase indexing, as explained in this forum post, works as follows:\nIt chunks your codebase files locally.\nThese chunks are then sent to Cursor\u0026rsquo;s server, where embeddings are created using either OpenAI\u0026rsquo;s embedding API or a custom embedding model.","keywords":["AI"],"articleBody":"There’s been a lot of buzz lately about Cursor, particularly its codebase indexing feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let’s dive in.\nUnderstanding Cursor’s Magic Cursor’s codebase indexing, as explained in this forum post, works as follows:\nIt chunks your codebase files locally.\nThese chunks are then sent to Cursor’s server, where embeddings are created using either OpenAI’s embedding API or a custom embedding model.\nThe embeddings, along with start/end line numbers and file paths, are stored in a remote vector database.\nWhen you use @Codebase or ⌘ Enter to ask about your codebase, Cursor retrieves relevant code chunks from this database to provide context for large language model (LLM) calls. In essence, Cursor employs a Retrieval-Augmented Generation (RAG) model, with the codebase index acting as the retrieval mechanism.\nBuilding Our Own Semantic Code Search Inspired by Cursor and Qdrant’s code search demo, we’ll replicate the codebase indexing feature and use it to power a semantic code search application. This application includes two main components:\nAn offline ingestion pipeline to index code embeddings into a vector database\nA code search server for semantic retrieval from this database\nLet’s break down each component and explore how we can implement them.\nIngestion Pipeline Our ingestion pipeline involves three crucial steps: splitting source code, creating embeddings, and indexing them in a vector database.\nWhy Split Source Code? Splitting source code files serves two primary purposes:\nOvercoming Model Input Limits: Embedding models have token limits. OpenAI’s text-embedding-3-small model, for example, has a token limit of 8192. Splitting ensures we stay within these bounds.\nEnhancing Semantic Granularity: Smaller chunks offer more precise semantic understanding. By focusing on specific parts of the code, we improve retrieval relevance and quality.\nSplitting Strategies While you could split code based on characters, words, or lines, a more sophisticated approach is to split based on tokens. We’ll use tiktoken, a fast Byte Pair Encoding (BPE) tokenizer compatible with OpenAI models.\nA naive strategy is to split code based on a fixed token count, but this can cut off code blocks like functions or classes mid-way. A more effective approach is to use an intelligent splitter that understands code structure, such as Langchain’s recursive text splitter. This method uses high-level delimiters (e.g., class and function definitions) to split at the appropriate semantic boundaries. However, this approach is language-specific and can struggles with languages that use curly braces for block delimitation.\nA even more elegant solution is to split the code based on its Abstract Syntax Tree (AST) structure, as outlined in this blog post. By traversing the AST depth-first, it splits code into sub-trees that fit within the token limits. To avoid creating too many small chunks, sibling nodes are merged into larger chunks as long as they stay under the token limit. LlamaIndex offers a clean Python implementation in its CodeSplitter function. Both implementations use tree-sitter for AST parsing, which supports a wide range of languages.\nWe use code-splitter (shameless plug: I’m the author!), a Rust re-implementation for added efficiency. Here is anan example of using its Python bindings to split Rust files in a directory:\nfrom code_splitter import Language, TiktokenSplitter def walk(dir: str, max_size: int) -\u003e Generator[dict[str, Any], None, None]: splitter = TiktokenSplitter(Language.Rust, max_size=max_size) for root, _, files in os.walk(dir): for file in files: if not file.endswith(\".rs\"): continue file_path = os.path.join(root, file) rel_path = os.path.relpath(file_path, dir) with open(file_path, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\") as f: lines = f.readlines() with open(file_path, mode=\"rb\") as f: code = f.read() chunks = splitter.split(code) for chunk in chunks: yield { \"file_path\": rel_path, \"file_name\": file, \"start_line\": chunk.start, \"end_line\": chunk.end, \"text\": \"\\n\".join(lines[chunk.start : chunk.end]), \"size\": chunk.size, } Creating Embeddings Qdrant’s authors used the open-source all-MiniLM-L6-v2 embedding model in their demo. Since this model is primarily trained on natural language tasks, they created a synthetic text-like representation of the code and passed it to the model. The representation captures key elements like function names, signatures, and docstrings.\nWe opted to use OpenAI’s text-embedding-3-small model. While not specifically trained on code, it performs reasonably well on code-related tasks. For those seeking more specialized alternatives, consider Microsoft’s unixcoder-base or Voyage AI’s voyage-code-2 which provides a much larger token limit.\nIndexing To index the code chunk embeddings, we’ll use Qdrant as in the original demo. Qdrant is an open-source vector database written in Rust and is optimized to handle high-dimensional vectors at scale. Here’s how we index our embeddings along with metadata such as file paths, start/end line numbers, and chunk sizes. This metadata enables the frontend to display relevant information during search results.\nimport pandas as pd from qdrant_client import QdrantClient from qdrant_client.models import Distance, PointStruct, VectorParams df = pd.read_parquet(\"/data/code_embeddings.parquet\") client = QdrantClient(\"http://localhost:6333\") client.recreate_collection( collection_name=\"qdrant-code\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) points = [ PointStruct( id=idx, vector=row[\"embedding\"].tolist(), payload=row.drop([\"embedding\"]).to_dict(), ) for idx, row in df.iterrows() ] client.upload_points(\"qdrant-code\", points) We’ll also index entire code files in a separate Qdrant collection, enabling full-file retrieval during search results.\nSemantic Code Search With our Qdrant database populated with code chunk embeddings and metadata, we can now build a code search server. Here’s the architecture of our search application:\nThe backend, built with FastAPI, handles REST requests and interacts with the Qdrant vector database. It exposes two endpoints:\nGET /api/search: Search for code snippets based on a query. GET /api/file: Fetch the full content of a file based on its path. For the frontend, we’ll reuse the React code from Qdrant’s demo. Below is an example of what a query might look like in the UI:\nWrapping Up We’ve successfully built a semantic code search application that mirrors Cursor’s codebase indexing functionality. This solution offers full control over each component—from code splitting and embedding generation to vector database indexing and search server implementation.\nReady to dive deeper? Check out the complete source code on GitHub: https://github.com/wangxj03/ai-cookbook/tree/main/code-search\nHappy coding, and may your semantic searches always find what you’re looking for!\n","wordCount":"984","inLanguage":"en","datePublished":"2024-09-24T23:01:05-07:00","dateModified":"2024-09-24T23:01:05-07:00","author":{"@type":"Person","name":"Xiaojing Wang"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2024-09-24-code-search/"},"publisher":{"@type":"Organization","name":"Xiaojing's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Semantic Code Search</h1><div class=post-meta><span title='2024-09-24 23:01:05 -0700 PDT'>September 24, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Xiaojing Wang</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#understanding-cursors-magic>Understanding Cursor&rsquo;s Magic</a></li><li><a href=#building-our-own-semantic-code-search>Building Our Own Semantic Code Search</a></li><li><a href=#ingestion-pipeline>Ingestion Pipeline</a><ul><li><a href=#why-split-source-code>Why Split Source Code?</a></li><li><a href=#creating-embeddings>Creating Embeddings</a></li><li><a href=#indexing>Indexing</a></li></ul></li><li><a href=#semantic-code-search>Semantic Code Search</a></li><li><a href=#wrapping-up>Wrapping Up</a></li></ul></nav></div></details></div><div class=post-content><p>There’s been a lot of buzz lately about Cursor, particularly its <a href=https://docs.cursor.com/context/codebase-indexing>codebase indexing</a> feature. This feature turns Cursor into a context-aware coding assistant. But how does it work, and can we build something similar? Let&rsquo;s dive in.</p><h2 id=understanding-cursors-magic>Understanding Cursor&rsquo;s Magic<a hidden class=anchor aria-hidden=true href=#understanding-cursors-magic>#</a></h2><p>Cursor&rsquo;s codebase indexing, as explained in this <a href=https://forum.cursor.com/t/codebase-indexing/36>forum post</a>, works as follows:</p><ol><li><p>It chunks your codebase files locally.</p></li><li><p>These chunks are then sent to Cursor&rsquo;s server, where embeddings are created using either OpenAI&rsquo;s embedding API or a custom embedding model.</p></li><li><p>The embeddings, along with start/end line numbers and file paths, are stored in a remote vector database.</p></li></ol><p>When you use @Codebase or ⌘ Enter to ask about your codebase, Cursor retrieves relevant code chunks from this database to provide context for large language model (LLM) calls. In essence, Cursor employs a Retrieval-Augmented Generation (RAG) model, with the codebase index acting as the retrieval mechanism.</p><h2 id=building-our-own-semantic-code-search>Building Our Own Semantic Code Search<a hidden class=anchor aria-hidden=true href=#building-our-own-semantic-code-search>#</a></h2><p>Inspired by Cursor and Qdrant’s <a href=https://github.com/qdrant/demo-code-search/tree/master>code search demo</a>, we&rsquo;ll replicate the codebase indexing feature and use it to power a semantic code search application. This application includes two main components:</p><ol><li><p>An offline ingestion pipeline to index code embeddings into a vector database</p></li><li><p>A code search server for semantic retrieval from this database</p></li></ol><p>Let&rsquo;s break down each component and explore how we can implement them.</p><h2 id=ingestion-pipeline>Ingestion Pipeline<a hidden class=anchor aria-hidden=true href=#ingestion-pipeline>#</a></h2><p>Our ingestion pipeline involves three crucial steps: splitting source code, creating embeddings, and indexing them in a vector database.</p><h3 id=why-split-source-code>Why Split Source Code?<a hidden class=anchor aria-hidden=true href=#why-split-source-code>#</a></h3><p>Splitting source code files serves two primary purposes:</p><ol><li><p><strong>Overcoming Model Input Limits</strong>: Embedding models have token limits. OpenAI&rsquo;s <a href=https://platform.openai.com/docs/guides/embeddings>text-embedding-3-small</a> model, for example, has a token limit of 8192. Splitting ensures we stay within these bounds.</p></li><li><p><strong>Enhancing Semantic Granularity</strong>: Smaller chunks offer more precise semantic understanding. By focusing on specific parts of the code, we improve retrieval relevance and quality.</p></li></ol><h4 id=splitting-strategies>Splitting Strategies<a hidden class=anchor aria-hidden=true href=#splitting-strategies>#</a></h4><p>While you could split code based on characters, words, or lines, a more sophisticated approach is to split based on tokens. We&rsquo;ll use <a href=https://github.com/openai/tiktoken>tiktoken</a>, a fast Byte Pair Encoding (BPE) tokenizer compatible with OpenAI models.</p><p>A naive strategy is to split code based on a fixed token count, but this can cut off code blocks like functions or classes mid-way. A more effective approach is to use an intelligent splitter that understands code structure, such as Langchain&rsquo;s <a href=https://python.langchain.com/docs/how_to/recursive_text_splitter/>recursive text splitter</a>. This method uses high-level delimiters (e.g., class and function definitions) to split at the appropriate semantic boundaries. However, this approach is language-specific and can struggles with languages that use curly braces for block delimitation.</p><p>A even more elegant solution is to split the code based on its Abstract Syntax Tree (AST) structure, as outlined in this <a href=https://docs.sweep.dev/blogs/chunking-2m-files>blog post</a>. By traversing the AST depth-first, it splits code into sub-trees that fit within the token limits. To avoid creating too many small chunks, sibling nodes are merged into larger chunks as long as they stay under the token limit. LlamaIndex offers a clean Python implementation in its <a href=https://docs.llamaindex.ai/en/v0.10.19/api/llama_index.core.node_parser.CodeSplitter.html>CodeSplitter</a> function. Both implementations use <a href=https://crates.io/crates/tree-sitter>tree-sitter</a> for AST parsing, which supports a wide range of languages.</p><p>We use <a href=https://github.com/wangxj03/code-splitter>code-splitter</a> (shameless plug: I&rsquo;m the author!), a Rust re-implementation for added efficiency. Here is anan example of using its <a href=https://pypi.org/project/code-splitter/>Python bindings</a> to split Rust files in a directory:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> code_splitter <span style=color:#f92672>import</span> Language, TiktokenSplitter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>walk</span>(dir: str, max_size: int) <span style=color:#f92672>-&gt;</span> Generator[dict[str, Any], <span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>None</span>]:
</span></span><span style=display:flex><span>    splitter <span style=color:#f92672>=</span> TiktokenSplitter(Language<span style=color:#f92672>.</span>Rust, max_size<span style=color:#f92672>=</span>max_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> root, _, files <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>walk(dir):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> file <span style=color:#f92672>in</span> files:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> file<span style=color:#f92672>.</span>endswith(<span style=color:#e6db74>&#34;.rs&#34;</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            file_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(root, file)
</span></span><span style=display:flex><span>            rel_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>relpath(file_path, dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> open(file_path, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>, errors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ignore&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>                lines <span style=color:#f92672>=</span> f<span style=color:#f92672>.</span>readlines()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> open(file_path, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>                code <span style=color:#f92672>=</span> f<span style=color:#f92672>.</span>read()
</span></span><span style=display:flex><span>                chunks <span style=color:#f92672>=</span> splitter<span style=color:#f92672>.</span>split(code)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> chunk <span style=color:#f92672>in</span> chunks:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>yield</span> {
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;file_path&#34;</span>: rel_path,
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;file_name&#34;</span>: file,
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;start_line&#34;</span>: chunk<span style=color:#f92672>.</span>start,
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;end_line&#34;</span>: chunk<span style=color:#f92672>.</span>end,
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(lines[chunk<span style=color:#f92672>.</span>start : chunk<span style=color:#f92672>.</span>end]),
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;size&#34;</span>: chunk<span style=color:#f92672>.</span>size,
</span></span><span style=display:flex><span>                    }
</span></span></code></pre></div><h3 id=creating-embeddings>Creating Embeddings<a hidden class=anchor aria-hidden=true href=#creating-embeddings>#</a></h3><p>Qdrant&rsquo;s authors used the open-source <a href=https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2>all-MiniLM-L6-v2</a> embedding model in their demo. Since this model is primarily trained on natural language tasks, they created a synthetic text-like representation of the code and passed it to the model. The representation captures key elements like function names, signatures, and docstrings.</p><p>We opted to use OpenAI’s <a href=https://platform.openai.com/docs/guides/embeddings>text-embedding-3-small</a> model. While not specifically trained on code, it performs reasonably well on code-related tasks. For those seeking more specialized alternatives, consider Microsoft’s <a href=https://huggingface.co/microsoft/unixcoder-base>unixcoder-base</a> or Voyage AI’s <a href=https://blog.voyageai.com/2024/01/23/voyage-code-2-elevate-your-code-retrieval/>voyage-code-2</a> which provides a much larger token limit.</p><h3 id=indexing>Indexing<a hidden class=anchor aria-hidden=true href=#indexing>#</a></h3><p>To index the code chunk embeddings, we&rsquo;ll use <a href=https://github.com/qdrant/qdrant>Qdrant</a> as in the original demo. Qdrant is an open-source vector database written in Rust and is optimized to handle high-dimensional vectors at scale. Here&rsquo;s how we index our embeddings along with metadata such as file paths, start/end line numbers, and chunk sizes. This metadata enables the frontend to display relevant information during search results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> qdrant_client <span style=color:#f92672>import</span> QdrantClient
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> qdrant_client.models <span style=color:#f92672>import</span> Distance, PointStruct, VectorParams
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#34;/data/code_embeddings.parquet&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> QdrantClient(<span style=color:#e6db74>&#34;http://localhost:6333&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client<span style=color:#f92672>.</span>recreate_collection(
</span></span><span style=display:flex><span>    collection_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;qdrant-code&#34;</span>,
</span></span><span style=display:flex><span>    vectors_config<span style=color:#f92672>=</span>VectorParams(size<span style=color:#f92672>=</span><span style=color:#ae81ff>1536</span>, distance<span style=color:#f92672>=</span>Distance<span style=color:#f92672>.</span>COSINE),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>points <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    PointStruct(
</span></span><span style=display:flex><span>        id<span style=color:#f92672>=</span>idx,
</span></span><span style=display:flex><span>        vector<span style=color:#f92672>=</span>row[<span style=color:#e6db74>&#34;embedding&#34;</span>]<span style=color:#f92672>.</span>tolist(),
</span></span><span style=display:flex><span>        payload<span style=color:#f92672>=</span>row<span style=color:#f92672>.</span>drop([<span style=color:#e6db74>&#34;embedding&#34;</span>])<span style=color:#f92672>.</span>to_dict(),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx, row <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>iterrows()
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client<span style=color:#f92672>.</span>upload_points(<span style=color:#e6db74>&#34;qdrant-code&#34;</span>, points)
</span></span></code></pre></div><p>We&rsquo;ll also index entire code files in a separate Qdrant collection, enabling full-file retrieval during search results.</p><h2 id=semantic-code-search>Semantic Code Search<a hidden class=anchor aria-hidden=true href=#semantic-code-search>#</a></h2><p>With our Qdrant database populated with code chunk embeddings and metadata, we can now build a code search server. Here&rsquo;s the architecture of our search application:</p><p><img loading=lazy src=code_search_design.svg alt></p><p>The backend, built with <a href=https://github.com/fastapi/fastapi>FastAPI</a>, handles REST requests and interacts with the Qdrant vector database. It exposes two endpoints:</p><ul><li><strong><code>GET /api/search</code></strong>: Search for code snippets based on a query.</li><li><strong><code>GET /api/file</code></strong>: Fetch the full content of a file based on its path.</li></ul><p>For the frontend, we&rsquo;ll reuse the <a href=https://github.com/qdrant/demo-code-search/blob/master/frontend>React code</a> from Qdrant&rsquo;s demo. Below is an example of what a query might look like in the UI:</p><p><img loading=lazy src=code_search_example.png alt></p><h2 id=wrapping-up>Wrapping Up<a hidden class=anchor aria-hidden=true href=#wrapping-up>#</a></h2><p>We&rsquo;ve successfully built a semantic code search application that mirrors Cursor&rsquo;s codebase indexing functionality. This solution offers full control over each component—from code splitting and embedding generation to vector database indexing and search server implementation.</p><p>Ready to dive deeper? Check out the complete source code on GitHub:
<a href=https://github.com/wangxj03/ai-cookbook/tree/main/code-search>https://github.com/wangxj03/ai-cookbook/tree/main/code-search</a></p><p>Happy coding, and may your semantic searches always find what you&rsquo;re looking for!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai/>AI</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/2024-08-23-anthropic-webui/><span class=title>Next »</span><br><span>Using Anthropic Models with Open WebUI</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on x" href="https://x.com/intent/tweet/?text=Semantic%20Code%20Search&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f&amp;hashtags=AI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f&amp;title=Semantic%20Code%20Search&amp;summary=Semantic%20Code%20Search&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f&title=Semantic%20Code%20Search"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on whatsapp" href="https://api.whatsapp.com/send?text=Semantic%20Code%20Search%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on telegram" href="https://telegram.me/share/url?text=Semantic%20Code%20Search&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Semantic Code Search on ycombinator" href="https://news.ycombinator.com/submitlink?t=Semantic%20Code%20Search&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-09-24-code-search%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Xiaojing's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>